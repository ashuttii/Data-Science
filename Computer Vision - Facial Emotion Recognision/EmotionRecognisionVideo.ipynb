{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EmotionRecognisionVideo.ipynb","provenance":[],"authorship_tag":"ABX9TyO5tw8K4ZuRsqj3/lvGf75c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Course Work - Facial Expression Recoginnision\n","\n","Module: Computer Vision - INM460\n","\n","By: Asha Guruvayurappan"],"metadata":{"id":"hHycOicxEUvL"}},{"cell_type":"markdown","source":["### **Facial Expression Recognission in Video**\n","\n","Implementing a series of image classification models using dataset from https://forms.gle/P7jK9TThhUjvUuvi9"],"metadata":{"id":"M4X9UkPHEWR7"}},{"cell_type":"markdown","source":["### Google Colab Setup\n","Before starting, we need to run a few commands to set up our environment on Google Colab. Also, if running this notebook on a local machine you can skip this section. \n","\n","The following cell is to mount the Google Drive"],"metadata":{"id":"UGx3_UoHEWUh"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CET89-wlEVZ7","executionInfo":{"status":"ok","timestamp":1652556962673,"user_tz":-60,"elapsed":13843,"user":{"displayName":"Asha G","userId":"11159914543598210133"}},"outputId":"f6573a7d-7cf8-4329-977e-7e37810937ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["To update openCV\n","\n"],"metadata":{"id":"rD4SmQ1cEVy_"}},{"cell_type":"code","source":["!pip install opencv-python==4.5.5.64"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VjgM5kN3Ej8g","executionInfo":{"status":"ok","timestamp":1652556991821,"user_tz":-60,"elapsed":12762,"user":{"displayName":"Asha G","userId":"11159914543598210133"}},"outputId":"516aef39-96b9-44b8-b45e-0b5fa9e53bab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting opencv-python==4.5.5.64\n","  Downloading opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n","\u001b[K     |████████████████████████████████| 60.5 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==4.5.5.64) (1.21.6)\n","Installing collected packages: opencv-python\n","  Attempting uninstall: opencv-python\n","    Found existing installation: opencv-python 4.1.2.30\n","    Uninstalling opencv-python-4.1.2.30:\n","      Successfully uninstalled opencv-python-4.1.2.30\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed opencv-python-4.5.5.64\n"]}]},{"cell_type":"markdown","source":["To check the version installed"],"metadata":{"id":"zj8HCfHrEoir"}},{"cell_type":"code","source":["!pip show opencv-python"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JT_04TO0Elkk","executionInfo":{"status":"ok","timestamp":1652557003267,"user_tz":-60,"elapsed":4034,"user":{"displayName":"Asha G","userId":"11159914543598210133"}},"outputId":"5e4aefb6-8e4d-466c-9217-625f6b8751a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Name: opencv-python\n","Version: 4.5.5.64\n","Summary: Wrapper package for OpenCV python bindings.\n","Home-page: https://github.com/skvark/opencv-python\n","Author: None\n","Author-email: None\n","License: MIT\n","Location: /usr/local/lib/python3.7/dist-packages\n","Requires: numpy\n","Required-by: imgaug, dopamine-rl, albumentations\n"]}]},{"cell_type":"markdown","source":["The path in the Google Drive to access the folders and files"],"metadata":{"id":"AR0yhBubEtVa"}},{"cell_type":"code","source":["import os\n","\n","# TODO: Fill in the Google Drive path where you uploaded the lab materials\n","# Example: GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab Notebooks/Lab materials 01-20210104'\n","\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'ColabNotebooks/ComputerVision/Course Work/CW_Folder_PG' \n","GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","print(os.listdir(GOOGLE_DRIVE_PATH))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"senl07s_Eqgz","executionInfo":{"status":"ok","timestamp":1652557021253,"user_tz":-60,"elapsed":786,"user":{"displayName":"Asha G","userId":"11159914543598210133"}},"outputId":"4bd982ff-6b6d-4b0f-8776-ad3219523efe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Code', '.DS_Store', 'Video', 'CW_Dataset', 'Models', 'test_functions.ipynb']\n"]}]},{"cell_type":"code","source":["import torch\n","import torch\n","import torchvision.models as models\n","import torch.nn as nn\n","\n","import os\n","import cv2\n","import numpy as np\n","from keras.preprocessing import image\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","from keras.preprocessing.image import load_img, img_to_array \n","from keras.models import  load_model\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from PIL import Image\n","import torchvision\n","from torchvision import datasets, models, transforms"],"metadata":{"id":"ZC656JQGEvsq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Assuming that we are on a CUDA machine, this should print a CUDA device:\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6jskRzdGFAW3","executionInfo":{"status":"ok","timestamp":1652557090731,"user_tz":-60,"elapsed":223,"user":{"displayName":"Asha G","userId":"11159914543598210133"}},"outputId":"10ba453e-7651-466d-dc5a-2f66f4f81af7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"markdown","source":["### **Load Model**\n","\n","The best CNN model saved in the drive is loaded."],"metadata":{"id":"XRYoBi2zFVsf"}},{"cell_type":"code","source":["path = GOOGLE_DRIVE_PATH+'/Models/CNN_model.pth'\n","model_param = torch.load(path, map_location=device)"],"metadata":{"id":"orGcQr5TE3hl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_ret = models.resnet18(pretrained=True)\n","num_ftrs = model_ret.fc.in_features\n","\n","model_ret.fc = nn.Linear(num_ftrs, 7)\n","model_ret.load_state_dict(model_param)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B8UQTOiWFCix","executionInfo":{"status":"ok","timestamp":1652557243206,"user_tz":-60,"elapsed":305,"user":{"displayName":"Asha G","userId":"11159914543598210133"}},"outputId":"76572a21-6c76-45b2-952f-109b580c5239"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":[""],"metadata":{"id":"hnyFtLUCFq_V"}},{"cell_type":"markdown","source":["The below code is to enable video cam to work in Google colab\n","\n","*Ref : https://colab.research.google.com/drive/1QnC7lV7oVFk5OZCm75fqbLAfD9qBy9bw?usp=sharing#scrollTo=Fj9YcAnsT4B_*"],"metadata":{"id":"4_7AC3w3HL8d"}},{"cell_type":"code","source":["from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","import cv2\n","import numpy as np\n","import PIL\n","import io\n","import html\n","import time"],"metadata":{"id":"wn3_ARLHF-u0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initialize the Haar Cascade face detection model\n","face_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))"],"metadata":{"id":"nYsa9bVzHCvb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Adapting to Google Colab\n","\n","The below lines of code is a javascript to adapt live webcam feed in google colab.\n","\n","***Run the below cells for the video to render***"],"metadata":{"id":"q3tB33bVPJhM"}},{"cell_type":"code","source":["# function to convert the JavaScript object into an OpenCV image\n","def js_to_image(js_reply):\n","  \"\"\"\n","  Params:\n","          js_reply: JavaScript object containing image from webcam\n","  Returns:\n","          img: OpenCV BGR image\n","  \"\"\"\n","  # decode base64 image\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  # convert bytes to numpy array\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  # decode numpy array into OpenCV BGR image\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","\n","  return img\n","\n","# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n","def bbox_to_bytes(bbox_array):\n","  \"\"\"\n","  Params:\n","          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n","  Returns:\n","        bytes: Base64 image byte string\n","  \"\"\"\n","  # convert array into PIL image\n","  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n","  iobuf = io.BytesIO()\n","  # format bbox into png for return\n","  bbox_PIL.save(iobuf, format='png')\n","  # format return string\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n","\n","  return bbox_bytes"],"metadata":{"id":"5oMvRB8GHCyX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# JavaScript to properly create our live video stream using our webcam as input\n","def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(js)\n","  \n","def video_frame(label, bbox):\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n","  return data"],"metadata":{"id":"oHQDDIvcHY3g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"_tiOA0WcHC3c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Transforms**\n","\n","\n","All transforms required for the model to predict"],"metadata":{"id":"sQjqGrEQHXat"}},{"cell_type":"code","source":["data_means = [0.485, 0.456, 0.406]\n","data_stds = [0.229, 0.224, 0.225]\n","transform = transforms.Compose([\n","        transforms.RandomResizedCrop(224),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(data_means, data_stds)\n","    ])"],"metadata":{"id":"HjhdwOq6HC6n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image as Img\n","from google.colab.patches import cv2_imshow"],"metadata":{"id":"tOgtrYZtIxQZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **VIDEO RENDERING**\n","\n","The below code renders live webcam video with face emotion recognision, and because cv2_imshow() in google colab renders as images each frame will be output as a image resulting in a series of images. \n","\n","**The CNN model from Google drive is loaded (above) and this model attempts to predict face emotions. Each frame of video is converted to image frame and this frames is transformed to the same way the CNN model was created**\n"],"metadata":{"id":"8zE5H4arOV8a"}},{"cell_type":"code","source":["# start streaming video from webcam\n","video_stream()\n","# label for video\n","label_html = 'Capturing...'\n","# initialze bounding box to empty\n","bbox = ''\n","count = 0 \n","frameNum = 0 \n","while True:\n","    frameNum +=1\n","    if (frameNum==20):\n","      break\n","    js_reply = video_frame(label_html, bbox)\n","    if not js_reply:\n","        break\n","\n","    # convert JS response to OpenCV Image\n","    img = js_to_image(js_reply[\"img\"])\n","\n","    # create transparent overlay for bounding box\n","    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n","\n","    # grayscale image for face detection\n","    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","\n","    # get face region coordinates\n","    faces = face_cascade.detectMultiScale(gray)\n","    # get face bounding box for overlay\n","    for (x,y,w,h) in faces:\n","      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n","      converted_image = Img.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","      trans_frame = transform(converted_image)\n","    outputs = model_ret(trans_frame.unsqueeze(0))\n","    _, preds = torch.max(outputs.data, 1)\n","    emo = {1: \"Surprise\", 2: \"Fear\", 3: \"Disgust\", 4: \"Happiness\", 5: \"Sadness\", 6: \"Anger\", 7: \"Neutral\"}\n","    # emo = {'1': \"Surprise\", '2': \"Fear\", '3': \"Disgust\", '4': \"Happiness\", '5': \"Sadness\", '6': \"Anger\", '7': \"Neutral\"}\n","    \n","    cv2.putText(img, emo[preds.item()], (100,100), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 0, 0),3)\n","    cv2_imshow(img)\n","\n","    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n","    # convert overlay of bbox into bytes\n","    bbox_bytes = bbox_to_bytes(bbox_array)\n","    # update bbox so next frame gets new overlay\n","    bbox = bbox_bytes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1TYV2GtqG7gmlok_Kepio2jkcMSrRPVnr"},"id":"xrHPGxd5Hva3","executionInfo":{"status":"ok","timestamp":1652559709354,"user_tz":-60,"elapsed":26430,"user":{"displayName":"Asha G","userId":"11159914543598210133"}},"outputId":"4b928ca2-b063-42b4-d083-b4f8db2943d6"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":[""],"metadata":{"id":"NNiG5TxVzumt"},"execution_count":null,"outputs":[]}]}